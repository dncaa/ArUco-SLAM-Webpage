<!DOCTYPE html>
<html lang="tr">
<head>
  <meta charset="UTF-8">
  <title>SLAM for Indoor Localization and Mapping on a Drone</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <nav class="topnav">
    <a class="active" href="#header">Introduction</a>
    <a href="#info">Information</a>
    <a href="#media">Gallery</a>
    <a href="poster.pdf">Poster</a>
    <a href="https://github.com/ecemsl/DuckieDroneSlam" target="_blank">GitHub</a>
  </nav>

  <div class="container">
    <header id="header">
      <h1>SLAM for Indoor Localization and Mapping on a Drone</h1>
      <p>This project focuses on the development of a GPS-free drone localization and mapping system using ArUco Tags and SLAM techniques. In environments where GPS is unreliable or unavailable such as indoors, underground, or in disaster-stricken areas, accurate and real-time localization remains a significant challenge. To address this, our system integrates camera-based ARTag detection with visual-inertial SLAM algorithms. The goal is to enable real-time, accurate drone localization and mapping using only onboard sensors.</p>
    </header>

    <section id="slam-explanation">
      <h2>What is EKF-SLAM?</h2>
      <p>Simultaneous Localization and Mapping (SLAM) is a technique that allows a robot or drone to build a map of an unknown environment while simultaneously tracking its own position within that map. In our project, we implemented an Extended Kalman Filter (EKF)-based SLAM system to enable a drone to navigate and localize itself indoors using only onboard sensors. The drone processes video footage to detect both visual features and ArUco markers in the environment. Visual odometry estimates the drone’s motion between frames, while detected markers are used to correct and refine its pose using EKF updates. The system incrementally builds a map of marker positions and produces a 3D trajectory of the drone’s path. This allows for accurate indoor localization even in GPS-denied environments.</p>
    </section>

    <section id="info">
      <h2>Project Details</h2>
      <h3>Hardware</h3>
      <h4>Duckiedrone21 Assembly</h4>
      <p>For this project, we used the DuckieDrone21 platform, which is a quadcopter specifically designed for indoor autonomy and research. The drone consists of a Raspberry Pi 3B+ as its onboard computer, connected to an ESC/Power board that supplies regulated power to the Pi and the motors. A flight controller is responsible for managing the low-level flight dynamics such as motor speeds, stabilization, and attitude control. It communicates with the Raspberry Pi, receiving high-level velocity commands and sending back IMU and state data. The flight controller ensures that the drone can maintain stable flight while the Raspberry Pi handles higher-level tasks such as SLAM and decision-making. The camera, mounted at the front of the drone, streams live video for SLAM processing.</p>
      
      <h4>Power Management</h4>
      <p>
      The drone is powered by a 3-cell Lithium Polymer (LiPo) battery. Proper battery management is essential to avoid discharge (below 10.5V) and overcharging, both of which can damage the battery. We monitored voltage levels during testing and used safe charging protocols to maintain battery health.
      </p>
      
      <h3>Software Environment</h3>
      <p>
      The onboard Raspberry Pi runs Ubuntu 18.04, with Robot Operating System (ROS) Melodic used for handling low-level communication and interfacing with the camera. The camera feed is published as a ROS topic, which can be visualized and used in real-time through the web interface.      </p>
      <h3>Flying DuckieDrone21</h3>
      <p>
      Flying the DuckieDrone21 begins with preparing the software environment by setting up a Docker workspace using the <code>pidrone_pkg</code> repository. After verifying that the drone’s battery is sufficiently charged, we connect to the drone's Wi-Fi network from the base station. Using the terminal, an SSH connection is established with the drone via <code>ssh duckie@amelia.local</code> (default password: <code>quackquack</code>).
      Once connected, we navigate to the <code>pidrone_pkg</code> directory and start the Docker container by executing <code>rake start</code>. A screen session is then launched using <code>screen -c pi.screenrc</code>, which initializes the necessary ROS nodes. 
      The web-based control interface is accessed by opening <code>index.html</code> located in <code>/pidrone_pkg/web/</code>, and loading it in a browser using the drone’s VLAN IP address as the hostname. Through this interface, we can monitor telemetry, view the camera feed, and send flight commands.
      Finally, the flight control system is activated by switching to the <code>1*$FC</code> screen tab and running <code>python flight_controller_node.py</code>. With the setup complete, the web interface can be used to arm the drone, select flight modes, and control its movement safely during SLAM experiments.
      </p>
      
    </section>

    <section id="media">
      <h2>Photos</h2>
      <img src="/images/drone-topdown.jpg" alt="Drone top down" width="300">

      <img src="/images/setup.jpg" alt="Setup" width="300">

      <h3>Video</h3>
      <video width="500" controls>
        <source src="/videos/tag2.mp4" type="video/mp4">
      </video>

      <video width="500" controls>
        <source src="/videos/tagdetection.mp4" type="video/mp4">
      </video>

      <video width="500" controls>
        <source src="/videos/webinterface.mp4" type="video/mp4">
      </video>
      
      <video width="500" controls>
        <source src="/videos/webinterface.mp4" type="video/mp4">
      </video>
    </section>

    <footer>
      2025 Hacettepe University
    </footer>
  </div>

</body>
</html>